{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3068da",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0acab9",
   "metadata": {},
   "source": [
    "create:\n",
    "Self-attention\n",
    "FFN\n",
    "Add & Norm\n",
    "Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c5069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinwang/main/GDGinNCKU_NLP_2025/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a2049",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b18c0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 10_000\n",
    "        self.max_length = 512\n",
    "        self.d_model = 512\n",
    "        self.num_heads = 8 # 在這ipynb會是 Multi-Head Attention(比起單純的Self-Attention複雜一些，但觀念相同)\n",
    "\n",
    "        self.d_ff = 2048 # Feed Forward\n",
    "\n",
    "        self.num_enc_layers = 6\n",
    "        self.num_dec_layers = 6\n",
    "\n",
    "        self.dropout = 0.1\n",
    "        self.pad_token_id = 0\n",
    "\n",
    "config = TransformerConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d35aacd",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$$\n",
    "PE_{pos, 2i} = sin(pos/10,000^{2i/d_{model}})\n",
    "$$\n",
    "$$\n",
    "PE_{pos, 2i+1} = cos(pos/10,000^{2i/d_{model}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f709c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length = 512, dropout = 0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model) # initilization\n",
    "\n",
    "        # 列出所有位置\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        # print(position)\n",
    "\n",
    "        # 計算P 10,000^{2i/d_{model}}\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -math.log(10000.0) / d_model)\n",
    "        # 套入公式\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe) # 保存位置訊息\n",
    "\n",
    "    # Encode 之後輸出！\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c5585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "max_seq_length = 100\n",
    "batch_size = 2\n",
    "seq_length = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32208a9",
   "metadata": {},
   "source": [
    "# Self Attention!\n",
    "\n",
    "Attention Sccores (What we learn on the class)\n",
    "$$\n",
    "scores = \\frac{Q \\cdot K^T}{\\sqrt{d_{model}}} \\cdot V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73a702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0 # 必須要能整除\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads \n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.DropOut(p=0.1)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k) # [batch_size, seq_length, num_heads, dim_k]\n",
    "        return x.transpose(1, 2) # [batch_size, num_heads, seq_length, dim_k]\n",
    "\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask = None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1) / math.sqrt(self.d_k))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        batch_size = query.size(0)\n",
    "        # 將輸入轉成多頭進行\n",
    "        Q = self.split_heads(self.W_q(query), batch_size)\n",
    "        K = self.split_heads(self.W_k(key), batch_size)\n",
    "        V = self.split_heads(self.W_v(value), batch_size)\n",
    "\n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # 合併多頭，順便轉換位置\n",
    "        # [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "        # [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        output = self.W_o(attn_output)\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4d1e0",
   "metadata": {},
   "source": [
    "## Feed Forward (簡單的DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9b48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_length, d_model]\n",
    "        x = self.linear1(x)      # [batch, seq_len, d_ff]\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)      # [batch, seq_len, d_model]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e96ba0",
   "metadata": {},
   "source": [
    "# Encoder (整合成Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ddc530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    單個 Encoder 層 = Multi-Head Attention + Feed-Forward\n",
    "    每個子層都有 Add & Norm（殘差連接 + 層歸一化）\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. Multi-Head Attention + Add & Norm\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))  # Add & Norm\n",
    "        \n",
    "        # 2. Feed-Forward + Add & Norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))    # Add & Norm\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b959483d",
   "metadata": {},
   "source": [
    "# Decoder (整合成Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1ff242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    單個 Decoder 層 = Masked Self-Attention + Cross-Attention + Feed-Forward\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        # 1. Masked Self-Attention (只能看到之前的詞)\n",
    "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # 2. Cross-Attention (關注 encoder 的輸出)\n",
    "        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(attn_output))\n",
    "        \n",
    "        # 3. Feed-Forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8d632",
   "metadata": {},
   "source": [
    "# Transformers (將Encoder, Decoder 結合！)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb133163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    完整的 Transformer 模型\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 src_vocab_size,      # 源語言詞彙表大小\n",
    "                 tgt_vocab_size,      # 目標語言詞彙表大小\n",
    "                 d_model=512,\n",
    "                 num_heads=8,\n",
    "                 num_encoder_layers=6,\n",
    "                 num_decoder_layers=6,\n",
    "                 d_ff=2048,\n",
    "                 max_seq_length=512,\n",
    "                 dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding 層\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
    "        \n",
    "        # Encoder 層堆疊\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder 層堆疊\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # 最後的線性層（投影到詞彙表）\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 初始化權重\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"初始化權重（使用 Xavier 初始化）\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"\n",
    "        生成 look-ahead mask（防止看到未來的詞）\n",
    "        上三角矩陣，對角線及以下為 1，上方為 0\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, 0)\n",
    "        return mask.unsqueeze(0).unsqueeze(0)  # [1, 1, sz, sz]\n",
    "    \n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"\n",
    "        創建源序列的 padding mask\n",
    "        src: [batch_size, src_seq_length]\n",
    "        \"\"\"\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        # [batch_size, 1, 1, src_seq_length]\n",
    "        return src_mask\n",
    "    \n",
    "    def make_tgt_mask(self, tgt):\n",
    "        \"\"\"\n",
    "        創建目標序列的 mask（padding mask + look-ahead mask）\n",
    "        tgt: [batch_size, tgt_seq_length]\n",
    "        \"\"\"\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        \n",
    "        # Padding mask\n",
    "        tgt_padding_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "        # [batch_size, 1, 1, tgt_seq_length]\n",
    "        \n",
    "        # Look-ahead mask\n",
    "        tgt_sub_mask = self.generate_square_subsequent_mask(tgt_len).to(tgt.device)\n",
    "        # [1, 1, tgt_seq_length, tgt_seq_length]\n",
    "        \n",
    "        # 結合兩種 mask\n",
    "        tgt_mask = tgt_padding_mask & tgt_sub_mask\n",
    "        return tgt_mask\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Encoder 前向傳播\n",
    "        \"\"\"\n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # 通過所有 Encoder 層\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Decoder 前向傳播\n",
    "        \"\"\"\n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # 通過所有 Decoder 層\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        完整前向傳播\n",
    "        Args:\n",
    "            src: [batch_size, src_seq_length] - 源序列\n",
    "            tgt: [batch_size, tgt_seq_length] - 目標序列\n",
    "        Returns:\n",
    "            output: [batch_size, tgt_seq_length, tgt_vocab_size]\n",
    "        \"\"\"\n",
    "        # 創建 mask\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # 投影到詞彙表\n",
    "        output = self.fc_out(decoder_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50eb67",
   "metadata": {},
   "source": [
    "# Inference\n",
    "* written by claude-4.5.sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc672c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "T5 (Text-to-Text Transfer Transformer) 推理示範\n",
      "============================================================\n",
      "\n",
      "📥 載入模型中...\n",
      "模型: t5-small\n",
      "使用設備: cpu\n",
      "\n",
      "📊 模型參數量: 60,506,624\n",
      "Encoder 層數: 6\n",
      "Decoder 層數: 6\n",
      "注意力頭數: 8\n",
      "d_model: 512\n",
      "d_ff: 2048\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"T5 (Text-to-Text Transfer Transformer) 推理示範\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================\n",
    "# 1. 載入模型和 Tokenizer\n",
    "# ============================================\n",
    "print(\"\\n📥 載入模型中...\")\n",
    "\n",
    "# 選擇模型大小（可以換成其他版本）\n",
    "# - t5-small: 60M 參數（最快，適合測試）\n",
    "# - t5-base: 220M 參數\n",
    "# - t5-large: 770M 參數\n",
    "# - t5-3b: 3B 參數\n",
    "\n",
    "model_name = \"t5-small\"  # 建議先用 small 測試\n",
    "print(f\"模型: {model_name}\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 如果有 GPU 就用 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 查看模型架構\n",
    "print(f\"\\n📊 模型參數量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Encoder 層數: {model.config.num_layers}\")\n",
    "print(f\"Decoder 層數: {model.config.num_decoder_layers}\")\n",
    "print(f\"注意力頭數: {model.config.num_heads}\")\n",
    "print(f\"d_model: {model.config.d_model}\")\n",
    "print(f\"d_ff: {model.config.d_ff}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. 定義推理函數\n",
    "# ============================================\n",
    "def generate_text(task_prefix, input_text, max_length=128):\n",
    "    \"\"\"\n",
    "    T5 的推理函數\n",
    "    \n",
    "    Args:\n",
    "        task_prefix: 任務前綴（T5 的特色）\n",
    "        input_text: 輸入文本\n",
    "        max_length: 最大生成長度\n",
    "    \"\"\"\n",
    "    # T5 的輸入格式：task_prefix + input_text\n",
    "    input_text = f\"{task_prefix}: {input_text}\"\n",
    "    \n",
    "    # Tokenization\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # 生成\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=4,           # Beam search\n",
    "        early_stopping=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b5ff518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "【任務 1: 英文翻譯成德文】\n",
      "輸入: Hello, how are you?\n",
      "輸出: Bonjour, comment êtes-vous?\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# 任務 1: 翻譯\n",
    "# ----------------\n",
    "print(\"\\n【任務 1: 英文翻譯成德文】\")\n",
    "input_text = \"Hello, how are you?\"\n",
    "task = \"translate English to French\"\n",
    "result = generate_text(task, input_text)\n",
    "print(f\"輸入: {input_text}\")\n",
    "print(f\"輸出: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2dda21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "【任務 2: 文本摘要】\n",
      "輸入: \n",
      "The Transformer architecture has revolutionized natural language processing. \n",
      "It was introduced in ...\n",
      "摘要: the Transformer architecture has revolutionized natural language processing. it was introduced in the paper 'Attention Is All You Need' by Vaswani et al.\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# 任務 2: 摘要\n",
    "# ----------------\n",
    "print(\"\\n【任務 2: 文本摘要】\")\n",
    "input_text = \"\"\"\n",
    "The Transformer architecture has revolutionized natural language processing. \n",
    "It was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017.\n",
    "The key innovation is the self-attention mechanism, which allows the model to weigh \n",
    "the importance of different words in a sentence. This architecture has become the \n",
    "foundation for models like BERT, GPT, and T5.\n",
    "\"\"\"\n",
    "task = \"summarize\"\n",
    "result = generate_text(task, input_text, max_length=50)\n",
    "print(f\"輸入: {input_text[:100]}...\")\n",
    "print(f\"摘要: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04a25c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "【任務 3: 問答】\n",
      "問題: When was the Transformer introduced?\n",
      "答案: 2017\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# 任務 3: 問答\n",
    "# ----------------\n",
    "print(\"\\n【任務 3: 問答】\")\n",
    "context = \"The Transformer was introduced in 2017 by Vaswani et al.\"\n",
    "question = \"When was the Transformer introduced?\"\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "task = \"\"  # 問答任務不需要前綴\n",
    "result = generate_text(task, input_text, max_length=20)\n",
    "print(f\"問題: {question}\")\n",
    "print(f\"答案: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34d77749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "【任務 4: 情感分類】\n",
      "輸入: This movie is absolutely wonderful! I loved every minute of it.\n",
      "情感: sentiment: This movie is absolutely wonderful!\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# 任務 4: 情感分類\n",
    "# ----------------\n",
    "print(\"\\n【任務 4: 情感分類】\")\n",
    "input_text = \"This movie is absolutely wonderful! I loved every minute of it.\"\n",
    "task = \"sentiment\"\n",
    "result = generate_text(task, input_text, max_length=10)\n",
    "print(f\"輸入: {input_text}\")\n",
    "print(f\"情感: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e52a9f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "模型內部結構\n",
      "============================================================\n",
      "\n",
      "【Encoder 結構】\n",
      "層數: 6\n",
      "第一層結構:\n",
      "  - Self-Attention: T5LayerSelfAttention\n",
      "  - Feed-Forward: T5LayerFF\n",
      "\n",
      "【Decoder 結構】\n",
      "層數: 6\n",
      "第一層結構:\n",
      "  - Self-Attention: T5LayerSelfAttention\n",
      "  - Cross-Attention: T5LayerCrossAttention\n",
      "  - Feed-Forward: T5LayerFF\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4. 查看內部結構（對比我們的實作）\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"模型內部結構\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n【Encoder 結構】\")\n",
    "print(f\"層數: {len(model.encoder.block)}\")\n",
    "print(f\"第一層結構:\")\n",
    "first_encoder_layer = model.encoder.block[0]\n",
    "print(f\"  - Self-Attention: {first_encoder_layer.layer[0].__class__.__name__}\")\n",
    "print(f\"  - Feed-Forward: {first_encoder_layer.layer[1].__class__.__name__}\")\n",
    "\n",
    "print(\"\\n【Decoder 結構】\")\n",
    "print(f\"層數: {len(model.decoder.block)}\")\n",
    "print(f\"第一層結構:\")\n",
    "first_decoder_layer = model.decoder.block[0]\n",
    "print(f\"  - Self-Attention: {first_decoder_layer.layer[0].__class__.__name__}\")\n",
    "print(f\"  - Cross-Attention: {first_decoder_layer.layer[1].__class__.__name__}\")\n",
    "print(f\"  - Feed-Forward: {first_decoder_layer.layer[2].__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59ba7d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "手動前向傳播展示\n",
      "============================================================\n",
      "\n",
      "輸入 token IDs 形狀: torch.Size([1, 8])\n",
      "Encoder 輸出形狀: torch.Size([1, 8, 512])\n",
      "  [batch_size, seq_length, d_model]\n",
      "Decoder 輸出形狀: torch.Size([1, 1, 512])\n",
      "最終 logits 形狀: torch.Size([1, 1, 32128])\n",
      "  [batch_size, seq_length, vocab_size]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5. 手動前向傳播（展示 Encoder-Decoder）\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"手動前向傳播展示\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "input_text = \"translate English to French: Hello world\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "print(f\"\\n輸入 token IDs 形狀: {input_ids.shape}\")\n",
    "\n",
    "# Encoder\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = model.encoder(input_ids)\n",
    "    encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "\n",
    "print(f\"Encoder 輸出形狀: {encoder_hidden_states.shape}\")\n",
    "print(f\"  [batch_size, seq_length, d_model]\")\n",
    "\n",
    "# Decoder（需要目標序列的開始）\n",
    "decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoder_outputs = model.decoder(\n",
    "        input_ids=decoder_input_ids,\n",
    "        encoder_hidden_states=encoder_hidden_states\n",
    "    )\n",
    "    decoder_hidden_states = decoder_outputs.last_hidden_state\n",
    "\n",
    "print(f\"Decoder 輸出形狀: {decoder_hidden_states.shape}\")\n",
    "\n",
    "# 投影到詞彙表\n",
    "with torch.no_grad():\n",
    "    lm_logits = model.lm_head(decoder_hidden_states)\n",
    "\n",
    "print(f\"最終 logits 形狀: {lm_logits.shape}\")\n",
    "print(f\"  [batch_size, seq_length, vocab_size]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00021f1d",
   "metadata": {},
   "source": [
    "與我們實作的對比\n",
    "\n",
    "我們的實作 vs T5:\n",
    "\n",
    "相同點 ✅:\n",
    "1. Encoder-Decoder 架構\n",
    "2. Multi-Head Attention\n",
    "3. Feed-Forward Network\n",
    "4. Positional Encoding（T5 用的是相對位置編碼）\n",
    "5. Layer Normalization\n",
    "6. 殘差連接\n",
    "\n",
    "差異點 🔍:\n",
    "1. T5 使用相對位置編碼，我們用絕對位置編碼\n",
    "2. T5 的 Layer Norm 位置略有不同（Pre-LN vs Post-LN）\n",
    "3. T5 使用 SentencePiece tokenizer\n",
    "4. T5 的訓練任務是 Text-to-Text\n",
    "5. T5 有多種大小的預訓練模型\n",
    "\n",
    "核心概念完全一樣！我們實作的就是 T5 的基礎架構。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea4766",
   "metadata": {},
   "source": [
    "💡 提示:\n",
    "1. 可以嘗試其他任務前綴：translate, summarize, question, etc.\n",
    "2. 可以換成其他模型：t5-base, t5-large, flan-t5-base\n",
    "3. 對於中文任務，可以使用 mT5（多語言版本）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdgnlp2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
