{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3068da",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0acab9",
   "metadata": {},
   "source": [
    "create:\n",
    "Self-attention\n",
    "FFN\n",
    "Add & Norm\n",
    "Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c5069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinwang/main/GDGinNCKU_NLP_2025/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a2049",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b18c0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 10_000\n",
    "        self.max_length = 512\n",
    "        self.d_model = 512\n",
    "        self.num_heads = 8 # åœ¨é€™ipynbæœƒæ˜¯ Multi-Head Attention(æ¯”èµ·å–®ç´”çš„Self-Attentionè¤‡é›œä¸€äº›ï¼Œä½†è§€å¿µç›¸åŒ)\n",
    "\n",
    "        self.d_ff = 2048 # Feed Forward\n",
    "\n",
    "        self.num_enc_layers = 6\n",
    "        self.num_dec_layers = 6\n",
    "\n",
    "        self.dropout = 0.1\n",
    "        self.pad_token_id = 0\n",
    "\n",
    "config = TransformerConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d35aacd",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$$\n",
    "PE_{pos, 2i} = sin(pos/10,000^{2i/d_{model}})\n",
    "$$\n",
    "$$\n",
    "PE_{pos, 2i+1} = cos(pos/10,000^{2i/d_{model}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f709c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length = 512, dropout = 0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model) # initilization\n",
    "\n",
    "        # åˆ—å‡ºæ‰€æœ‰ä½ç½®\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        # print(position)\n",
    "\n",
    "        # è¨ˆç®—P 10,000^{2i/d_{model}}\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -math.log(10000.0) / d_model)\n",
    "        # å¥—å…¥å…¬å¼\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe) # ä¿å­˜ä½ç½®è¨Šæ¯\n",
    "\n",
    "    # Encode ä¹‹å¾Œè¼¸å‡ºï¼\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c5585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "max_seq_length = 100\n",
    "batch_size = 2\n",
    "seq_length = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32208a9",
   "metadata": {},
   "source": [
    "# Self Attention!\n",
    "\n",
    "Attention Sccores (What we learn on the class)\n",
    "$$\n",
    "scores = softmax(\\frac{Q \\cdot K^T}{\\sqrt{d_{model}}}) \\cdot V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73a702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0 # å¿…é ˆè¦èƒ½æ•´é™¤\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads \n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.DropOut(p=0.1)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k) # [batch_size, seq_length, num_heads, dim_k]\n",
    "        return x.transpose(1, 2) # [batch_size, num_heads, seq_length, dim_k]\n",
    "\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask = None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1) / math.sqrt(self.d_k))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        batch_size = query.size(0)\n",
    "        # å°‡è¼¸å…¥è½‰æˆå¤šé ­é€²è¡Œ\n",
    "        Q = self.split_heads(self.W_q(query), batch_size)\n",
    "        K = self.split_heads(self.W_k(key), batch_size)\n",
    "        V = self.split_heads(self.W_v(value), batch_size)\n",
    "\n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # åˆä½µå¤šé ­ï¼Œé †ä¾¿è½‰æ›ä½ç½®\n",
    "        # [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "        # [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        output = self.W_o(attn_output)\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4d1e0",
   "metadata": {},
   "source": [
    "## Feed Forward (ç°¡å–®çš„DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9b48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_length, d_model]\n",
    "        x = self.linear1(x)      # [batch, seq_len, d_ff]\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)      # [batch, seq_len, d_model]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e96ba0",
   "metadata": {},
   "source": [
    "# Encoder (æ•´åˆæˆEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ddc530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    å–®å€‹ Encoder å±¤ = Multi-Head Attention + Feed-Forward\n",
    "    æ¯å€‹å­å±¤éƒ½æœ‰ Add & Normï¼ˆæ®˜å·®é€£æ¥ + å±¤æ­¸ä¸€åŒ–ï¼‰\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. Multi-Head Attention + Add & Norm\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))  # Add & Norm\n",
    "        \n",
    "        # 2. Feed-Forward + Add & Norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))    # Add & Norm\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b959483d",
   "metadata": {},
   "source": [
    "# Decoder (æ•´åˆæˆDecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1ff242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    å–®å€‹ Decoder å±¤ = Masked Self-Attention + Cross-Attention + Feed-Forward\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        # 1. Masked Self-Attention (åªèƒ½çœ‹åˆ°ä¹‹å‰çš„è©)\n",
    "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # 2. Cross-Attention (é—œæ³¨ encoder çš„è¼¸å‡º)\n",
    "        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(attn_output))\n",
    "        \n",
    "        # 3. Feed-Forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8d632",
   "metadata": {},
   "source": [
    "# Transformers (å°‡Encoder, Decoder çµåˆï¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb133163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„ Transformer æ¨¡å‹\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 src_vocab_size,      # æºèªè¨€è©å½™è¡¨å¤§å°\n",
    "                 tgt_vocab_size,      # ç›®æ¨™èªè¨€è©å½™è¡¨å¤§å°\n",
    "                 d_model=512,\n",
    "                 num_heads=8,\n",
    "                 num_encoder_layers=6,\n",
    "                 num_decoder_layers=6,\n",
    "                 d_ff=2048,\n",
    "                 max_seq_length=512,\n",
    "                 dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding å±¤\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
    "        \n",
    "        # Encoder å±¤å †ç–Š\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder å±¤å †ç–Š\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # æœ€å¾Œçš„ç·šæ€§å±¤ï¼ˆæŠ•å½±åˆ°è©å½™è¡¨ï¼‰\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # åˆå§‹åŒ–æ¬Šé‡\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"åˆå§‹åŒ–æ¬Šé‡ï¼ˆä½¿ç”¨ Xavier åˆå§‹åŒ–ï¼‰\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆ look-ahead maskï¼ˆé˜²æ­¢çœ‹åˆ°æœªä¾†çš„è©ï¼‰\n",
    "        ä¸Šä¸‰è§’çŸ©é™£ï¼Œå°è§’ç·šåŠä»¥ä¸‹ç‚º 1ï¼Œä¸Šæ–¹ç‚º 0\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, 0)\n",
    "        return mask.unsqueeze(0).unsqueeze(0)  # [1, 1, sz, sz]\n",
    "    \n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"\n",
    "        å‰µå»ºæºåºåˆ—çš„ padding mask\n",
    "        src: [batch_size, src_seq_length]\n",
    "        \"\"\"\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        # [batch_size, 1, 1, src_seq_length]\n",
    "        return src_mask\n",
    "    \n",
    "    def make_tgt_mask(self, tgt):\n",
    "        \"\"\"\n",
    "        å‰µå»ºç›®æ¨™åºåˆ—çš„ maskï¼ˆpadding mask + look-ahead maskï¼‰\n",
    "        tgt: [batch_size, tgt_seq_length]\n",
    "        \"\"\"\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        \n",
    "        # Padding mask\n",
    "        tgt_padding_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "        # [batch_size, 1, 1, tgt_seq_length]\n",
    "        \n",
    "        # Look-ahead mask\n",
    "        tgt_sub_mask = self.generate_square_subsequent_mask(tgt_len).to(tgt.device)\n",
    "        # [1, 1, tgt_seq_length, tgt_seq_length]\n",
    "        \n",
    "        # çµåˆå…©ç¨® mask\n",
    "        tgt_mask = tgt_padding_mask & tgt_sub_mask\n",
    "        return tgt_mask\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Encoder å‰å‘å‚³æ’­\n",
    "        \"\"\"\n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # é€šéæ‰€æœ‰ Encoder å±¤\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Decoder å‰å‘å‚³æ’­\n",
    "        \"\"\"\n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # é€šéæ‰€æœ‰ Decoder å±¤\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        å®Œæ•´å‰å‘å‚³æ’­\n",
    "        Args:\n",
    "            src: [batch_size, src_seq_length] - æºåºåˆ—\n",
    "            tgt: [batch_size, tgt_seq_length] - ç›®æ¨™åºåˆ—\n",
    "        Returns:\n",
    "            output: [batch_size, tgt_seq_length, tgt_vocab_size]\n",
    "        \"\"\"\n",
    "        # å‰µå»º mask\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # æŠ•å½±åˆ°è©å½™è¡¨\n",
    "        output = self.fc_out(decoder_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50eb67",
   "metadata": {},
   "source": [
    "# Inference\n",
    "* written by claude-4.5.sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc672c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "T5 (Text-to-Text Transfer Transformer) æ¨ç†ç¤ºç¯„\n",
      "============================================================\n",
      "\n",
      "ğŸ“¥ è¼‰å…¥æ¨¡å‹ä¸­...\n",
      "æ¨¡å‹: t5-small\n",
      "ä½¿ç”¨è¨­å‚™: cpu\n",
      "\n",
      "ğŸ“Š æ¨¡å‹åƒæ•¸é‡: 60,506,624\n",
      "Encoder å±¤æ•¸: 6\n",
      "Decoder å±¤æ•¸: 6\n",
      "æ³¨æ„åŠ›é ­æ•¸: 8\n",
      "d_model: 512\n",
      "d_ff: 2048\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"T5 (Text-to-Text Transfer Transformer) æ¨ç†ç¤ºç¯„\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================\n",
    "# 1. è¼‰å…¥æ¨¡å‹å’Œ Tokenizer\n",
    "# ============================================\n",
    "print(\"\\nğŸ“¥ è¼‰å…¥æ¨¡å‹ä¸­...\")\n",
    "\n",
    "# é¸æ“‡æ¨¡å‹å¤§å°ï¼ˆå¯ä»¥æ›æˆå…¶ä»–ç‰ˆæœ¬ï¼‰\n",
    "# - t5-small: 60M åƒæ•¸ï¼ˆæœ€å¿«ï¼Œé©åˆæ¸¬è©¦ï¼‰\n",
    "# - t5-base: 220M åƒæ•¸\n",
    "# - t5-large: 770M åƒæ•¸\n",
    "# - t5-3b: 3B åƒæ•¸\n",
    "\n",
    "model_name = \"t5-small\"  # å»ºè­°å…ˆç”¨ small æ¸¬è©¦\n",
    "print(f\"æ¨¡å‹: {model_name}\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# å¦‚æœæœ‰ GPU å°±ç”¨ GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# æŸ¥çœ‹æ¨¡å‹æ¶æ§‹\n",
    "print(f\"\\nğŸ“Š æ¨¡å‹åƒæ•¸é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Encoder å±¤æ•¸: {model.config.num_layers}\")\n",
    "print(f\"Decoder å±¤æ•¸: {model.config.num_decoder_layers}\")\n",
    "print(f\"æ³¨æ„åŠ›é ­æ•¸: {model.config.num_heads}\")\n",
    "print(f\"d_model: {model.config.d_model}\")\n",
    "print(f\"d_ff: {model.config.d_ff}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. å®šç¾©æ¨ç†å‡½æ•¸\n",
    "# ============================================\n",
    "def generate_text(task_prefix, input_text, max_length=128):\n",
    "    \"\"\"\n",
    "    T5 çš„æ¨ç†å‡½æ•¸\n",
    "    \n",
    "    Args:\n",
    "        task_prefix: ä»»å‹™å‰ç¶´ï¼ˆT5 çš„ç‰¹è‰²ï¼‰\n",
    "        input_text: è¼¸å…¥æ–‡æœ¬\n",
    "        max_length: æœ€å¤§ç”Ÿæˆé•·åº¦\n",
    "    \"\"\"\n",
    "    # T5 çš„è¼¸å…¥æ ¼å¼ï¼štask_prefix + input_text\n",
    "    input_text = f\"{task_prefix}: {input_text}\"\n",
    "    \n",
    "    # Tokenization\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # ç”Ÿæˆ\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=4,           # Beam search\n",
    "        early_stopping=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b5ff518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ã€ä»»å‹™ 1: è‹±æ–‡ç¿»è­¯æˆå¾·æ–‡ã€‘\n",
      "è¼¸å…¥: Hello, how are you?\n",
      "è¼¸å‡º: Bonjour, comment Ãªtes-vous?\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# ä»»å‹™ 1: ç¿»è­¯\n",
    "# ----------------\n",
    "print(\"\\nã€ä»»å‹™ 1: è‹±æ–‡ç¿»è­¯æˆå¾·æ–‡ã€‘\")\n",
    "input_text = \"Hello, how are you?\"\n",
    "task = \"translate English to French\"\n",
    "result = generate_text(task, input_text)\n",
    "print(f\"è¼¸å…¥: {input_text}\")\n",
    "print(f\"è¼¸å‡º: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2dda21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ã€ä»»å‹™ 2: æ–‡æœ¬æ‘˜è¦ã€‘\n",
      "è¼¸å…¥: \n",
      "The Transformer architecture has revolutionized natural language processing. \n",
      "It was introduced in ...\n",
      "æ‘˜è¦: the Transformer architecture has revolutionized natural language processing. it was introduced in the paper 'Attention Is All You Need' by Vaswani et al.\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# ä»»å‹™ 2: æ‘˜è¦\n",
    "# ----------------\n",
    "print(\"\\nã€ä»»å‹™ 2: æ–‡æœ¬æ‘˜è¦ã€‘\")\n",
    "input_text = \"\"\"\n",
    "The Transformer architecture has revolutionized natural language processing. \n",
    "It was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017.\n",
    "The key innovation is the self-attention mechanism, which allows the model to weigh \n",
    "the importance of different words in a sentence. This architecture has become the \n",
    "foundation for models like BERT, GPT, and T5.\n",
    "\"\"\"\n",
    "task = \"summarize\"\n",
    "result = generate_text(task, input_text, max_length=50)\n",
    "print(f\"è¼¸å…¥: {input_text[:100]}...\")\n",
    "print(f\"æ‘˜è¦: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04a25c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ã€ä»»å‹™ 3: å•ç­”ã€‘\n",
      "å•é¡Œ: When was the Transformer introduced?\n",
      "ç­”æ¡ˆ: 2017\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# ä»»å‹™ 3: å•ç­”\n",
    "# ----------------\n",
    "print(\"\\nã€ä»»å‹™ 3: å•ç­”ã€‘\")\n",
    "context = \"The Transformer was introduced in 2017 by Vaswani et al.\"\n",
    "question = \"When was the Transformer introduced?\"\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "task = \"\"  # å•ç­”ä»»å‹™ä¸éœ€è¦å‰ç¶´\n",
    "result = generate_text(task, input_text, max_length=20)\n",
    "print(f\"å•é¡Œ: {question}\")\n",
    "print(f\"ç­”æ¡ˆ: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34d77749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ã€ä»»å‹™ 4: æƒ…æ„Ÿåˆ†é¡ã€‘\n",
      "è¼¸å…¥: This movie is absolutely wonderful! I loved every minute of it.\n",
      "æƒ…æ„Ÿ: sentiment: This movie is absolutely wonderful!\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# ä»»å‹™ 4: æƒ…æ„Ÿåˆ†é¡\n",
    "# ----------------\n",
    "print(\"\\nã€ä»»å‹™ 4: æƒ…æ„Ÿåˆ†é¡ã€‘\")\n",
    "input_text = \"This movie is absolutely wonderful! I loved every minute of it.\"\n",
    "task = \"sentiment\"\n",
    "result = generate_text(task, input_text, max_length=10)\n",
    "print(f\"è¼¸å…¥: {input_text}\")\n",
    "print(f\"æƒ…æ„Ÿ: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e52a9f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "æ¨¡å‹å…§éƒ¨çµæ§‹\n",
      "============================================================\n",
      "\n",
      "ã€Encoder çµæ§‹ã€‘\n",
      "å±¤æ•¸: 6\n",
      "ç¬¬ä¸€å±¤çµæ§‹:\n",
      "  - Self-Attention: T5LayerSelfAttention\n",
      "  - Feed-Forward: T5LayerFF\n",
      "\n",
      "ã€Decoder çµæ§‹ã€‘\n",
      "å±¤æ•¸: 6\n",
      "ç¬¬ä¸€å±¤çµæ§‹:\n",
      "  - Self-Attention: T5LayerSelfAttention\n",
      "  - Cross-Attention: T5LayerCrossAttention\n",
      "  - Feed-Forward: T5LayerFF\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4. æŸ¥çœ‹å…§éƒ¨çµæ§‹ï¼ˆå°æ¯”æˆ‘å€‘çš„å¯¦ä½œï¼‰\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æ¨¡å‹å…§éƒ¨çµæ§‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nã€Encoder çµæ§‹ã€‘\")\n",
    "print(f\"å±¤æ•¸: {len(model.encoder.block)}\")\n",
    "print(f\"ç¬¬ä¸€å±¤çµæ§‹:\")\n",
    "first_encoder_layer = model.encoder.block[0]\n",
    "print(f\"  - Self-Attention: {first_encoder_layer.layer[0].__class__.__name__}\")\n",
    "print(f\"  - Feed-Forward: {first_encoder_layer.layer[1].__class__.__name__}\")\n",
    "\n",
    "print(\"\\nã€Decoder çµæ§‹ã€‘\")\n",
    "print(f\"å±¤æ•¸: {len(model.decoder.block)}\")\n",
    "print(f\"ç¬¬ä¸€å±¤çµæ§‹:\")\n",
    "first_decoder_layer = model.decoder.block[0]\n",
    "print(f\"  - Self-Attention: {first_decoder_layer.layer[0].__class__.__name__}\")\n",
    "print(f\"  - Cross-Attention: {first_decoder_layer.layer[1].__class__.__name__}\")\n",
    "print(f\"  - Feed-Forward: {first_decoder_layer.layer[2].__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59ba7d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "æ‰‹å‹•å‰å‘å‚³æ’­å±•ç¤º\n",
      "============================================================\n",
      "\n",
      "è¼¸å…¥ token IDs å½¢ç‹€: torch.Size([1, 8])\n",
      "Encoder è¼¸å‡ºå½¢ç‹€: torch.Size([1, 8, 512])\n",
      "  [batch_size, seq_length, d_model]\n",
      "Decoder è¼¸å‡ºå½¢ç‹€: torch.Size([1, 1, 512])\n",
      "æœ€çµ‚ logits å½¢ç‹€: torch.Size([1, 1, 32128])\n",
      "  [batch_size, seq_length, vocab_size]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5. æ‰‹å‹•å‰å‘å‚³æ’­ï¼ˆå±•ç¤º Encoder-Decoderï¼‰\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æ‰‹å‹•å‰å‘å‚³æ’­å±•ç¤º\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "input_text = \"translate English to French: Hello world\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "print(f\"\\nè¼¸å…¥ token IDs å½¢ç‹€: {input_ids.shape}\")\n",
    "\n",
    "# Encoder\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = model.encoder(input_ids)\n",
    "    encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "\n",
    "print(f\"Encoder è¼¸å‡ºå½¢ç‹€: {encoder_hidden_states.shape}\")\n",
    "print(f\"  [batch_size, seq_length, d_model]\")\n",
    "\n",
    "# Decoderï¼ˆéœ€è¦ç›®æ¨™åºåˆ—çš„é–‹å§‹ï¼‰\n",
    "decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    decoder_outputs = model.decoder(\n",
    "        input_ids=decoder_input_ids,\n",
    "        encoder_hidden_states=encoder_hidden_states\n",
    "    )\n",
    "    decoder_hidden_states = decoder_outputs.last_hidden_state\n",
    "\n",
    "print(f\"Decoder è¼¸å‡ºå½¢ç‹€: {decoder_hidden_states.shape}\")\n",
    "\n",
    "# æŠ•å½±åˆ°è©å½™è¡¨\n",
    "with torch.no_grad():\n",
    "    lm_logits = model.lm_head(decoder_hidden_states)\n",
    "\n",
    "print(f\"æœ€çµ‚ logits å½¢ç‹€: {lm_logits.shape}\")\n",
    "print(f\"  [batch_size, seq_length, vocab_size]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00021f1d",
   "metadata": {},
   "source": [
    "èˆ‡æˆ‘å€‘å¯¦ä½œçš„å°æ¯”\n",
    "\n",
    "æˆ‘å€‘çš„å¯¦ä½œ vs T5:\n",
    "\n",
    "ç›¸åŒé» âœ…:\n",
    "1. Encoder-Decoder æ¶æ§‹\n",
    "2. Multi-Head Attention\n",
    "3. Feed-Forward Network\n",
    "4. Positional Encodingï¼ˆT5 ç”¨çš„æ˜¯ç›¸å°ä½ç½®ç·¨ç¢¼ï¼‰\n",
    "5. Layer Normalization\n",
    "6. æ®˜å·®é€£æ¥\n",
    "\n",
    "å·®ç•°é» ğŸ”:\n",
    "1. T5 ä½¿ç”¨ç›¸å°ä½ç½®ç·¨ç¢¼ï¼Œæˆ‘å€‘ç”¨çµ•å°ä½ç½®ç·¨ç¢¼\n",
    "2. T5 çš„ Layer Norm ä½ç½®ç•¥æœ‰ä¸åŒï¼ˆPre-LN vs Post-LNï¼‰\n",
    "3. T5 ä½¿ç”¨ SentencePiece tokenizer\n",
    "4. T5 çš„è¨“ç·´ä»»å‹™æ˜¯ Text-to-Text\n",
    "5. T5 æœ‰å¤šç¨®å¤§å°çš„é è¨“ç·´æ¨¡å‹\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µå®Œå…¨ä¸€æ¨£ï¼æˆ‘å€‘å¯¦ä½œçš„å°±æ˜¯ T5 çš„åŸºç¤æ¶æ§‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea4766",
   "metadata": {},
   "source": [
    "ğŸ’¡ æç¤º:\n",
    "1. å¯ä»¥å˜—è©¦å…¶ä»–ä»»å‹™å‰ç¶´ï¼štranslate, summarize, question, etc.\n",
    "2. å¯ä»¥æ›æˆå…¶ä»–æ¨¡å‹ï¼št5-base, t5-large, flan-t5-base\n",
    "3. å°æ–¼ä¸­æ–‡ä»»å‹™ï¼Œå¯ä»¥ä½¿ç”¨ mT5ï¼ˆå¤šèªè¨€ç‰ˆæœ¬ï¼‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdgnlp2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
