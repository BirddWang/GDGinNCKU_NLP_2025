{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c6fd5e6",
   "metadata": {},
   "source": [
    "# Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559af112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinwang/main/GDGinNCKU_NLP_2025/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe455274",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd6c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    編碼器 (Encoder)\n",
    "    將嵌入後的序列編碼成一個固定長度的上下文向量 (context vector)\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, hidden_dim, n_layers=1, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # LSTM 層：處理序列資料\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, \n",
    "                          dropout=dropout if n_layers > 1 else 0,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, embedded):\n",
    "        \"\"\"\n",
    "        參數:\n",
    "            embedded: 嵌入後的輸入序列 [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        回傳:\n",
    "            outputs: 所有時間步的輸出 [batch_size, seq_len, hidden_dim]\n",
    "            hidden: 最後的隱藏狀態 (h_n, c_n)\n",
    "        \"\"\"\n",
    "        # embedded shape: [batch_size, seq_len, emb_dim]\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs shape: [batch_size, seq_len, hidden_dim]\n",
    "        # hidden: (h_n, c_n) 各為 [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea6f44",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7916472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    解碼器 (Decoder)\n",
    "    根據編碼器的上下文向量，逐步生成輸出序列\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers=1, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # LSTM 層\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers,\n",
    "                          dropout=dropout if n_layers > 1 else 0,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        # 輸出層：將隱藏狀態映射到詞彙表大小\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, embedded, hidden):\n",
    "        \"\"\"\n",
    "        參數:\n",
    "            embedded: 嵌入後的當前時間步輸入 [batch_size, 1, emb_dim]\n",
    "            hidden: 上一時間步的隱藏狀態 (h, c)\n",
    "        \n",
    "        回傳:\n",
    "            prediction: 預測的詞彙分布 [batch_size, output_dim]\n",
    "            hidden: 更新後的隱藏狀態\n",
    "        \"\"\"\n",
    "        # embedded shape: [batch_size, 1, emb_dim]\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # output shape: [batch_size, 1, hidden_dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        # prediction shape: [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121aab03",
   "metadata": {},
   "source": [
    "## Encoder - Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bebccd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    序列到序列模型 (Sequence-to-Sequence Model)\n",
    "    結合嵌入層、編碼器和解碼器，完成序列轉換任務\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, emb_dim, hidden_dim, n_layers, dropout, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        # 嵌入層\n",
    "        self.src_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.trg_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        # 編碼器和解碼器\n",
    "        self.encoder = Encoder(emb_dim, hidden_dim, n_layers, dropout)\n",
    "        self.decoder = Decoder(output_dim, emb_dim, hidden_dim, n_layers, dropout)\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        參數:\n",
    "            src: 來源序列 [batch_size, src_len]\n",
    "            trg: 目標序列 [batch_size, trg_len]\n",
    "            teacher_forcing_ratio: 教師強迫比例（訓練時使用真實標籤的機率）\n",
    "        \n",
    "        回傳:\n",
    "            outputs: 預測結果 [batch_size, trg_len, output_dim]\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # 儲存解碼器的輸出\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # 來源序列的嵌入\n",
    "        src_embedded = self.src_embedding(src)\n",
    "        # src_embedded shape: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # 編碼器處理來源序列\n",
    "        enc_outputs, hidden = self.encoder(src_embedded)\n",
    "        \n",
    "        # 解碼器的第一個輸入（通常是 <SOS> 起始符號）\n",
    "        input = trg[:, 0].unsqueeze(1)  # [batch_size, 1]\n",
    "        \n",
    "        # 逐步生成目標序列\n",
    "        for t in range(1, trg_len):\n",
    "            # 目標序列的嵌入\n",
    "            trg_embedded = self.trg_embedding(input)\n",
    "            # trg_embedded shape: [batch_size, 1, emb_dim]\n",
    "            \n",
    "            output, hidden = self.decoder(trg_embedded, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            # 決定下一個輸入：使用真實標籤或模型預測\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            input = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f27c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定參數\n",
    "INPUT_DIM = 1024   # 來源詞彙表大小\n",
    "OUTPUT_DIM = 1024  # 目標詞彙表大小\n",
    "EMB_DIM = 256      # 嵌入維度\n",
    "HIDDEN_DIM = 512   # 隱藏層維度\n",
    "N_LAYERS = 2       # LSTM 層數\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150306ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已建立，使用設備: mps\n",
      "\n",
      "模型架構:\n",
      "Seq2Seq(\n",
      "  (src_embedding): Embedding(1024, 256)\n",
      "  (trg_embedding): Embedding(1024, 256)\n",
      "  (encoder): Encoder(\n",
      "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (fc_out): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "總參數量: 8,406,016\n",
      "可訓練參數量: 8,406,016\n",
      "\n",
      "輸入形狀: torch.Size([32, 10])\n",
      "目標形狀: torch.Size([32, 12])\n",
      "輸出形狀: torch.Size([32, 12, 1024])\n",
      "\n",
      "✓ 模型測試成功！\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(INPUT_DIM, OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT, device).to(device)\n",
    "\n",
    "print(f\"模型已建立，使用設備: {device}\")\n",
    "print(f\"\\n模型架構:\")\n",
    "print(model)\n",
    "\n",
    "# 計算參數量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n總參數量: {total_params:,}\")\n",
    "print(f\"可訓練參數量: {trainable_params:,}\")\n",
    "\n",
    "# 測試前向傳播\n",
    "batch_size = 32\n",
    "src_len = 10\n",
    "trg_len = 12\n",
    "\n",
    "src = torch.randint(0, INPUT_DIM, (batch_size, src_len)).to(device)\n",
    "trg = torch.randint(0, OUTPUT_DIM, (batch_size, trg_len)).to(device)\n",
    "\n",
    "output = model(src, trg, teacher_forcing_ratio=0.5)\n",
    "print(f\"\\n輸入形狀: {src.shape}\")\n",
    "print(f\"目標形狀: {trg.shape}\")\n",
    "print(f\"輸出形狀: {output.shape}\")\n",
    "print(\"\\n✓ 模型測試成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837da738",
   "metadata": {},
   "source": [
    "# 近代Encoder-Decoder (2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2eee816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5GemmaConfig\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"google/t5gemma-b-b-prefixlm\"\n",
    "device = \"mps\" # mps, cuda, cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5adc7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = T5GemmaConfig.from_pretrained(MODEL_NAME)\n",
    "model_config.num_hidden_layers = 3 ## 這是這模型目前的bug 不加這行跑不了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb005ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, config = model_config).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb5eb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5GemmaForConditionalGeneration(\n",
      "  (model): T5GemmaModel(\n",
      "    (encoder): T5GemmaEncoder(\n",
      "      (embed_tokens): Embedding(256000, 768, padding_idx=0)\n",
      "      (norm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "      (rotary_emb): T5GemmaRotaryEmbedding()\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x T5GemmaEncoderLayer(\n",
      "          (self_attn): T5GemmaSelfAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          )\n",
      "          (pre_self_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "          (post_self_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "          (mlp): T5GemmaMLP(\n",
      "            (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
      "            (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
      "            (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
      "            (act_fn): GELUTanh()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (pre_feedforward_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "          (post_feedforward_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (decoder): T5GemmaDecoder(\n",
      "      (embed_tokens): Embedding(256000, 768, padding_idx=0)\n",
      "      (norm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "      (rotary_emb): T5GemmaRotaryEmbedding()\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x T5GemmaDecoderLayer(\n",
      "          (self_attn): T5GemmaSelfAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          )\n",
      "          (pre_self_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "          (post_self_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "          (mlp): T5GemmaMLP(\n",
      "            (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
      "            (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
      "            (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
      "            (act_fn): GELUTanh()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (pre_feedforward_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "          (post_feedforward_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (cross_attn): T5GemmaCrossAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          )\n",
      "          (pre_cross_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "          (post_cross_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): T5GemmaLMHead(\n",
      "    (out_proj): Linear(in_features=768, out_features=256000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ec177c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, Glad to see you.\", return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67d0032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>\n",
      "\n",
      "I'm glad to hear you're doing well.\n",
      "\n",
      "I'm glad to hear you're doing well.\n",
      "\n",
      "I'm glad to hear you're doing well.\n",
      "\n",
      "I'm glad to hear you're doing well.<eos>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_length=96, num_beams=8, early_stopping=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668d5847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdgnlp2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
